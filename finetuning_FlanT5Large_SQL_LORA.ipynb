{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (0.17.0)\n",
      "Requirement already satisfied: nltk in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: datasets in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (4.0.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (1.9.0)\n",
      "Requirement already satisfied: transformers[torch] in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (4.54.1)\n",
      "Requirement already satisfied: textstat in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (0.7.8)\n",
      "Requirement already satisfied: sentence-transformers in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (5.1.0)\n",
      "Requirement already satisfied: bert_score in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (0.3.13)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from peft) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from peft) (2.7.1)\n",
      "Requirement already satisfied: tqdm in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from peft) (0.34.3)\n",
      "Requirement already satisfied: click in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from nltk) (8.2.2)\n",
      "Requirement already satisfied: joblib in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: filelock in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from datasets) (2025.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from transformers[torch]) (0.21.4)\n",
      "Requirement already satisfied: pyphen in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from textstat) (0.17.2)\n",
      "Requirement already satisfied: cmudict in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from textstat) (1.1.1)\n",
      "Requirement already satisfied: setuptools in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from textstat) (65.5.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: Pillow in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: matplotlib in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from bert_score) (3.10.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: importlib-metadata>=5 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from cmudict->textstat) (8.7.0)\n",
      "Requirement already satisfied: importlib-resources>=5 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from cmudict->textstat) (6.5.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from matplotlib->bert_score) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from matplotlib->bert_score) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from matplotlib->bert_score) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from matplotlib->bert_score) (3.2.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from importlib-metadata>=5->cmudict->textstat) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install peft nltk datasets sentencepiece 'accelerate>=0.26.0' \"transformers[torch]\" textstat sentence-transformers bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/pushking/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/pushking/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pushking/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Use PyTorch with MPS backend\n",
    "import torch\n",
    "import random\n",
    "import nltk\n",
    "import textstat\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For Flan T5 Large\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import pipeline\n",
    "from collections import Counter\n",
    "from bert_score import score\n",
    "# Load the pretrained T5 model (not fine-tuned)\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "# Loading PEFT configs\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset for evaluation\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gretelai/synthetic_text_to_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing pretrained model for Apple MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining method for calculating evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple ROUGE implementation without external dependencies\n",
    "def calculate_rouge_scores(reference, candidate):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE-1, ROUGE-2, ROUGE-L scores\n",
    "    \"\"\"\n",
    "    ref_tokens = set(nltk.word_tokenize(reference.lower()))\n",
    "    cand_tokens = set(nltk.word_tokenize(candidate.lower()))\n",
    "    \n",
    "    # ROUGE-1\n",
    "    rouge1 = len(ref_tokens.intersection(cand_tokens)) / len(ref_tokens) if ref_tokens else 0\n",
    "    \n",
    "    # ROUGE-2 (bigrams)\n",
    "    ref_bigrams = set(zip(nltk.word_tokenize(reference.lower())[:-1], \n",
    "                          nltk.word_tokenize(reference.lower())[1:]))\n",
    "    cand_bigrams = set(zip(nltk.word_tokenize(candidate.lower())[:-1], \n",
    "                           nltk.word_tokenize(candidate.lower())[1:]))\n",
    "    rouge2 = len(ref_bigrams.intersection(cand_bigrams)) / len(ref_bigrams) if ref_bigrams else 0\n",
    "    \n",
    "    # ROUGE-L (longest common subsequence)\n",
    "    def lcs_length(s1, s2):\n",
    "        m, n = len(s1), len(s2)\n",
    "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "        for i in range(1, m + 1):\n",
    "            for j in range(1, n + 1):\n",
    "                if s1[i-1] == s2[j-1]:\n",
    "                    dp[i][j] = dp[i-1][j-1] + 1\n",
    "                else:\n",
    "                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "        return dp[m][n]\n",
    "    \n",
    "    ref_words = nltk.word_tokenize(reference.lower())\n",
    "    cand_words = nltk.word_tokenize(candidate.lower())\n",
    "    lcs = lcs_length(ref_words, cand_words)\n",
    "    rouge_l = lcs / len(ref_words) if ref_words else 0\n",
    "    \n",
    "    return rouge1, rouge2, rouge_l\n",
    "\n",
    "# Calculate METEOR score\n",
    "def calculate_meteor(reference_sql, generated_sql):\n",
    "    ref_tokens = [word_tokenize(reference_sql.lower())]\n",
    "    gen_tokens = word_tokenize(generated_sql.lower())\n",
    "    score = meteor_score(ref_tokens, gen_tokens)\n",
    "    return score\n",
    "\n",
    "# Calculate GLEU score\n",
    "def calculate_gleu(reference_sql, generated_sql):\n",
    "    ref_tokens = word_tokenize(reference_sql.lower())\n",
    "    gen_tokens = word_tokenize(generated_sql.lower())\n",
    "        \n",
    "    # Calculate n-gram overlaps\n",
    "    def get_ngrams(tokens, n):\n",
    "        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "        \n",
    "    # GLEU calculation (simplified version)\n",
    "    ref_1grams = set(get_ngrams(ref_tokens, 1))\n",
    "    gen_1grams = set(get_ngrams(gen_tokens, 1))\n",
    "        \n",
    "    precision = len(ref_1grams.intersection(gen_1grams)) / len(gen_1grams) if gen_1grams else 0\n",
    "    recall = len(ref_1grams.intersection(gen_1grams)) / len(ref_1grams) if ref_1grams else 0\n",
    "        \n",
    "    gleu = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return gleu\n",
    "\n",
    "# Calculate Fleash Reading Ease score which measures readability and complexity of the SQL\n",
    "def calculate_flesch_reading_ease(generated_sql):\n",
    "    # Convert SQL to more readable format for scoring\n",
    "    readable_sql = generated_sql.replace('SELECT', 'SELECT ').replace('FROM', ' FROM ')\n",
    "    flesch_score = textstat.flesch_reading_ease(readable_sql)\n",
    "    return flesch_score\n",
    "\n",
    "# Calculate BERT score\n",
    "def calculate_bert_score(reference_sql, generated_sql):\n",
    "    try: \n",
    "        bert_score = score([generated_sql], [reference_sql], lang='en', verbose=False)\n",
    "        return bert_score[0].item()   # Return F1 scores\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BERT score for {generated_sql}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Calculate CoSIM score\n",
    "def calculate_cosim_score(reference_sql, generated_sql):\n",
    "    sentence_transformer_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    reference_embeddings = sentence_transformer_model.encode(reference_sql)\n",
    "    generated_embeddings = sentence_transformer_model.encode(generated_sql)\n",
    "    # Calculate cosine similarity using dot product\n",
    "    ref_norm = reference_embeddings / np.linalg.norm(reference_embeddings)\n",
    "    gen_norm = generated_embeddings / np.linalg.norm(generated_embeddings)\n",
    "    \n",
    "    cosim_score = np.dot(ref_norm, gen_norm)\n",
    "    return cosim_score\n",
    "\n",
    "def calculate_toxicity_score(generated_sql):\n",
    "    \"\"\"\n",
    "    Calculate toxicity score for a single SQL query\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize toxicity analyzer\n",
    "        toxicity_analyzer = pipeline(\n",
    "            \"text-classification\", \n",
    "            model=\"unitary/toxic-bert\", \n",
    "            return_all_scores=True\n",
    "        )\n",
    "        \n",
    "        # Analyze toxicity of the SQL query\n",
    "        results = toxicity_analyzer(generated_sql)\n",
    "        # Get the maximum toxicity score across all categories\n",
    "        # Categories: toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
    "        max_toxicity = max([score['score'] for score in results[0]])\n",
    "        \n",
    "        return max_toxicity\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating toxicity score: {e}\")\n",
    "        # Return 0 if toxicity calculation fails\n",
    "        return 0.0\n",
    "\n",
    "# Evaluation function - REPLACE YOUR EXISTING FUNCTION WITH THIS\n",
    "def evaluate_sql_generation(model, tokenizer, test_samples=50, sample_dataset=None):\n",
    "    \"\"\"\n",
    "    Evaluate T5 Large pretrained model on SQL generation\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'bleu_scores': [],\n",
    "        'rouge_1_scores': [],\n",
    "        'rouge_2_scores': [],\n",
    "        'rouge_l_scores': [],\n",
    "        'meteor_scores': [],\n",
    "        'gleu_scores': [],\n",
    "        'flesch_reading_ease_scores': [],  \n",
    "        'bert_scores': [],\n",
    "        'cosim_scores': [],\n",
    "        'repetition_rates': [],\n",
    "        'novelty_scores': [],\n",
    "        'diversity_scores': [],\n",
    "        'toxicity_scores': []\n",
    "    }\n",
    "    \n",
    "    # Get test samples\n",
    "    if test_samples > 0:\n",
    "        test_data = dataset['test'].select(range(min(test_samples, len(dataset['test']))))\n",
    "    else:\n",
    "        test_data = sample_dataset\n",
    "    \n",
    "    generated_sqls = []\n",
    "    reference_sqls = []\n",
    "    prompts = []\n",
    "    \n",
    "    for i, example in enumerate(test_data):\n",
    "        # Prepare input\n",
    "        input_text = f\"Question: {example['sql_prompt']} Context: {example['sql_context']}, IMPORTANT: With or without the context, your answer must always be a SQL statement\", \n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate SQL\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=256,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "        \n",
    "        generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        reference_sql = example['sql']\n",
    "        \n",
    "        generated_sqls.append(generated_sql)\n",
    "        reference_sqls.append(reference_sql)\n",
    "        prompts.append(example['sql_prompt'])\n",
    "    \n",
    "        \n",
    "        # Calculate metrics\n",
    "        # 1. BLEU Score\n",
    "        reference_tokens = nltk.word_tokenize(reference_sql.lower())\n",
    "        generated_tokens = nltk.word_tokenize(generated_sql.lower())\n",
    "        bleu_score = sentence_bleu([reference_tokens], generated_tokens, \n",
    "                                 smoothing_function=SmoothingFunction().method1)\n",
    "        results['bleu_scores'].append(bleu_score)\n",
    "        \n",
    "        # 2. ROUGE Scores\n",
    "        rouge1, rouge2, rouge_l = calculate_rouge_scores(reference_sql, generated_sql)\n",
    "        results['rouge_1_scores'].append(rouge1)\n",
    "        results['rouge_2_scores'].append(rouge2)\n",
    "        results['rouge_l_scores'].append(rouge_l)\n",
    "\n",
    "        meteor_score = calculate_meteor(reference_sql, generated_sql)\n",
    "        results['meteor_scores'].append(meteor_score)\n",
    "\n",
    "        # 3. GLEU Score\n",
    "        gleu_score = calculate_gleu(reference_sql, generated_sql)\n",
    "        results['gleu_scores'].append(gleu_score)\n",
    "\n",
    "        # 4. Fleash Reading Ease Score\n",
    "        flesch_reading_ease_score = calculate_flesch_reading_ease(generated_sql)\n",
    "        results['flesch_reading_ease_scores'].append(flesch_reading_ease_score)\n",
    "\n",
    "        # 5. BERT Score\n",
    "        bert_score = calculate_bert_score(reference_sql, generated_sql)\n",
    "        results['bert_scores'].append(bert_score)\n",
    "\n",
    "        # 6. CoSIM Score\n",
    "        cosim_score = calculate_cosim_score(reference_sql, generated_sql)\n",
    "        results['cosim_scores'].append(cosim_score)\n",
    "        \n",
    "        # 3. Repetition Rate\n",
    "        # It is the percentage of tokens that are repeated in the generated SQL\n",
    "        tokens = generated_sql.split()\n",
    "        if len(tokens) > 0:\n",
    "            repetition_rate = 1 - len(set(tokens)) / len(tokens)\n",
    "        else:\n",
    "            repetition_rate = 0\n",
    "        results['repetition_rates'].append(repetition_rate)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processed {i+1}/{len(test_data)} samples\")\n",
    "    \n",
    "    # Calculate novelty and diversity across all generated SQLs\n",
    "    # To calculate novelty, we need to count the frequency of each token in the generated SQLs\n",
    "    # Then we can calculate the average novelty of the generated SQLs\n",
    "\n",
    "    all_tokens = []\n",
    "    for sql in generated_sqls:\n",
    "        all_tokens.extend(nltk.word_tokenize(sql.lower()))\n",
    "    \n",
    "    # 4. Novelty (how different from common patterns)\n",
    "    token_freq = Counter(all_tokens)\n",
    "    novelty_scores = []\n",
    "    for sql in generated_sqls:\n",
    "        tokens = nltk.word_tokenize(sql.lower())\n",
    "        avg_novelty = np.mean([1 / (token_freq.get(token, 1) + 1) for token in tokens])\n",
    "        novelty_scores.append(avg_novelty)\n",
    "    results['novelty_scores'] = novelty_scores\n",
    "    \n",
    "    # 5. Diversity (unique SQL patterns)\n",
    "    unique_patterns = len(set(generated_sqls))\n",
    "    diversity_score = unique_patterns / len(generated_sqls)\n",
    "    results['diversity_scores'] = [diversity_score] * len(generated_sqls)\n",
    "\n",
    "    # 6. Toxicity Score\n",
    "    toxicity_scores = [calculate_toxicity_score(sql) for sql in generated_sqls]\n",
    "    results['toxicity_scores'] = toxicity_scores\n",
    "    \n",
    "    return results, generated_sqls, reference_sqls, test_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing PEFT using LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 2,359,296 || all params: 785,509,376 || trainable%: 0.3004\n",
      "Number of Trainable parameters: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                    # Rank of low-rank matrices\n",
    "    lora_alpha=16,           # Scaling factor\n",
    "    target_modules=[\"q\", \"v\"],  # Target all attention components\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM #Seq2SeqLM for T5\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.to(device)\n",
    "print(f\"Number of Trainable parameters: {model.print_trainable_parameters()}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data:   0%|          | 0/20000 [00:00<?, ? examples/s]/Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3950: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Processing training data: 100%|██████████| 20000/20000 [00:07<00:00, 2693.68 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing validation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation data: 100%|██████████| 1170/1170 [00:00<00:00, 2711.01 examples/s]\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 20000\n",
      "Validation dataset size: 1170\n"
     ]
    }
   ],
   "source": [
    "#Preparing train and test datasets\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset for Text-to-SQL generation\n",
    "    \"\"\"\n",
    "    # Format input as instruction\n",
    "    inputs = [f\"Question: {prompt} Context: {context}\" \n",
    "                 for prompt, context in zip(examples['sql_prompt'], examples['sql_context'])]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets (SQL queries)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples['sql'],\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    # Replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "    model_inputs[\"labels\"] = torch.where(\n",
    "        model_inputs[\"labels\"] == tokenizer.pad_token_id,\n",
    "        -100,\n",
    "        model_inputs[\"labels\"]\n",
    "    )\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Updated data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "# Prepare datasets\n",
    "print(\"Preparing training dataset...\")\n",
    "train_indices = random.sample(range(len(dataset['train'])), int(0.2 * len(dataset['train'])))\n",
    "train_dataset = dataset['train'].select(train_indices).map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    "    desc=\"Processing training data\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Preparing validation dataset...\")\n",
    "val_indices = random.sample(range(len(dataset['test'])), int(0.2 * len(dataset['test'])))\n",
    "val_dataset = dataset['test'].select(val_indices).map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['test'].column_names,\n",
    "    desc=\"Processing validation data\"\n",
    ")\n",
    "\n",
    "# Debug dataset structure\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Training arguments with epochs and evaluation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./flan-t5-large-sql-lora\",\n",
    "    num_train_epochs=2,  # Train for 3 epochs\n",
    "    eval_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",  # Save after each epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2, # Further reduced batch size\n",
    "    per_device_eval_batch_size=2,\n",
    "    #warmup_steps=50,\n",
    "    #logging_steps=5,\n",
    "    save_total_limit=2,  # Keep last 3 checkpoints\n",
    "    #load_best_model_at_end=True,\n",
    "    #metric_for_best_model=\"eval_loss\",\n",
    "    #greater_is_better=False,\n",
    "    #dataloader_pin_memory=False,\n",
    "    # remove_unused_columns=False, # Removed this line\n",
    "    # Add evaluation metrics\n",
    "    #eval_steps=None,  # Remove this when using epoch strategy,\n",
    "    # Add these for speed\n",
    "    #dataloader_num_workers=0,       # Disable multiprocessing\n",
    "    report_to=None,\n",
    "    #gradient_checkpointing=True # Enable gradient checkpointing\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fine-tuning\n",
    "#print(\"Starting LoRA fine-tuning...\")\n",
    "#trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "#trainer.save_model(\"./flan-t5-large-sql-lora-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmarking finetuned model on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from checkpoint!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/50 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 11/50 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 21/50 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 31/50 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error calculating BERT score for SELECT organization_id, COUNT(*) FROM medical_supplies WHERE location = 'East Africa' AND distribution_date >= DATE_SUB(CURRENT_DATE, INTERVAL 3 YEAR);: (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /api/models/roberta-large/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x3ad605810>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 32b0c3d3-15fd-4e0a-8c5b-3d6af6838d66)')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 41/50 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n",
      "/Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS FOR T5 LARGE FINETUNED WITH LORA MODEL\n",
      "==================================================\n",
      "Bleu Scores: 0.4121 ± 0.3298\n",
      "Rouge 1 Scores: 0.7223 ± 0.2065\n",
      "Rouge 2 Scores: 0.5072 ± 0.2925\n",
      "Rouge L Scores: 0.6483 ± 0.2501\n",
      "Meteor Scores: 0.6026 ± 0.2729\n",
      "Gleu Scores: 0.7534 ± 0.1712\n",
      "Flesch Reading Ease Scores: 15.9710 ± 43.4595\n",
      "Bert Scores: 0.9234 ± 0.1379\n",
      "Cosim Scores: 0.8977 ± 0.0854\n",
      "Repetition Rates: 0.0093 ± 0.0256\n",
      "Novelty Scores: 0.1782 ± 0.0392\n",
      "Diversity Scores: 1.0000 ± 0.0000\n",
      "Toxicity Scores: 0.0007 ± 0.0006\n",
      "\n",
      "==================================================\n",
      "SAMPLE GENERATIONS\n",
      "==================================================\n",
      "\n",
      "Example 1:\n",
      "Actual: SELECT AVG(explainability_score) FROM creative_ai WHERE region IN ('Europe', 'North America');\n",
      "SQL Prompt: What is the average explainability score of creative AI applications in 'Europe' and 'North America' in the 'creative_ai' table?\n",
      "SQL Context: CREATE TABLE creative_ai (application_id INT, name TEXT, region TEXT, explainability_score FLOAT); INSERT INTO creative_ai (application_id, name, region, explainability_score) VALUES (1, 'ApplicationX', 'Europe', 0.87), (2, 'ApplicationY', 'North America', 0.91), (3, 'ApplicationZ', 'Europe', 0.84), (4, 'ApplicationAA', 'North America', 0.93), (5, 'ApplicationAB', 'Europe', 0.89);\n",
      "Generated: SELECT AVG(explainability_score) FROM creative_ai WHERE region IN ('Europe', 'North America');\n",
      "BLEU: 1.0000\n",
      "ROUGE-L: 1.0000\n",
      "METEOR: 0.9999\n",
      "GLEU: 1.0000\n",
      "Flesch Reading Ease: 2.1050\n",
      "BERT: 1.0000\n",
      "CoSIM: 1.0000\n",
      "Repetition Rate: 0.0000\n",
      "Novelty: 0.1725\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0006\n",
      "\n",
      "Example 2:\n",
      "Actual: DELETE FROM rural_infrastructure WHERE country = 'Indonesia' AND completion_date < '2010-01-01';\n",
      "SQL Prompt: Delete all records of rural infrastructure projects in Indonesia that have a completion date before 2010.\n",
      "SQL Context: CREATE TABLE rural_infrastructure (id INT, project_name TEXT, sector TEXT, country TEXT, completion_date DATE); INSERT INTO rural_infrastructure (id, project_name, sector, country, completion_date) VALUES (1, 'Water Supply Expansion', 'Infrastructure', 'Indonesia', '2008-05-15'), (2, 'Rural Electrification', 'Infrastructure', 'Indonesia', '2012-08-28'), (3, 'Transportation Improvement', 'Infrastructure', 'Indonesia', '2009-12-31');\n",
      "Generated: DELETE FROM rural_infrastructure WHERE country = 'Indonesia' AND completion_date  2010;\n",
      "BLEU: 0.6996\n",
      "ROUGE-L: 0.7857\n",
      "METEOR: 0.7779\n",
      "GLEU: 0.8800\n",
      "Flesch Reading Ease: 0.3000\n",
      "BERT: 0.9755\n",
      "CoSIM: 0.9822\n",
      "Repetition Rate: 0.0000\n",
      "Novelty: 0.1984\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0007\n",
      "\n",
      "Example 3:\n",
      "Actual: SELECT launch_provider, COUNT(*) FROM Accidents GROUP BY launch_provider;\n",
      "SQL Prompt: How many accidents have been recorded for SpaceX and Blue Origin rocket launches?\n",
      "SQL Context: CREATE TABLE Accidents (id INT, launch_provider VARCHAR(255), year INT, description TEXT); INSERT INTO Accidents (id, launch_provider, year, description) VALUES (1, 'SpaceX', 2015, 'Falcon 9 explosion'), (2, 'Blue Origin', 2011, 'Propulsion system failure'), (3, 'SpaceX', 2016, 'Falcon 9 explosion');\n",
      "Generated: SELECT COUNT(*) FROM Accidents WHERE launch_provider IN ('SpaceX', 'Blue Origin');\n",
      "BLEU: 0.2834\n",
      "ROUGE-L: 0.6923\n",
      "METEOR: 0.4673\n",
      "GLEU: 0.7143\n",
      "Flesch Reading Ease: 61.3250\n",
      "BERT: 0.8883\n",
      "CoSIM: 0.8375\n",
      "Repetition Rate: 0.0000\n",
      "Novelty: 0.1656\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0006\n",
      "\n",
      "Example 4:\n",
      "Actual: SELECT MAX(quantity) FROM sales;\n",
      "SQL Prompt: What is the maximum quantity of seafood sold in a single transaction?\n",
      "SQL Context: CREATE TABLE sales (id INT, location VARCHAR(20), quantity INT, price DECIMAL(5,2)); INSERT INTO sales (id, location, quantity, price) VALUES (1, 'Northeast', 50, 12.99), (2, 'Midwest', 75, 19.99), (3, 'West', 120, 14.49);\n",
      "Generated: SELECT MAX(quantity) FROM sales;\n",
      "BLEU: 1.0000\n",
      "ROUGE-L: 1.0000\n",
      "METEOR: 0.9990\n",
      "GLEU: 1.0000\n",
      "Flesch Reading Ease: 54.7250\n",
      "BERT: 1.0000\n",
      "CoSIM: 1.0000\n",
      "Repetition Rate: 0.0000\n",
      "Novelty: 0.1345\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0006\n",
      "\n",
      "Example 5:\n",
      "Actual: SELECT SUM(budget) FROM Movies_Release_Year WHERE release_year < 2010;\n",
      "SQL Prompt: What is the total budget for movies released before 2010?\n",
      "SQL Context: CREATE TABLE Movies_Release_Year (id INT, title VARCHAR(100), release_year INT, budget DECIMAL(10,2)); INSERT INTO Movies_Release_Year (id, title, release_year, budget) VALUES (1, 'The Matrix', 1999, 63000000.00), (2, 'Titanic', 1997, 200000000.00), (3, 'The Lord of the Rings: The Fellowship of the Ring', 2001, 93000000.00);\n",
      "Generated: SELECT SUM(budget) FROM Movies_Release_Year WHERE release_year  2010;\n",
      "BLEU: 0.7772\n",
      "ROUGE-L: 0.9167\n",
      "METEOR: 0.9216\n",
      "GLEU: 0.9565\n",
      "Flesch Reading Ease: 54.7014\n",
      "BERT: 0.9767\n",
      "CoSIM: 0.9963\n",
      "Repetition Rate: 0.0000\n",
      "Novelty: 0.1712\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0006\n",
      "\n",
      "Example 6:\n",
      "Actual: INSERT INTO attorneys (attorney_name, attorney_email, attorney_phone) VALUES ('Oliver Martinez', 'oliver.martinez@lawfirm.com', '555-555-5678');\n",
      "SQL Prompt: Add a new attorney named 'Oliver Martinez' with email 'oliver.martinez@lawfirm.com' and phone '555-555-5678' to the 'attorneys' table\n",
      "SQL Context: CREATE TABLE attorneys (attorney_id INT, attorney_name VARCHAR(50), attorney_email VARCHAR(50), attorney_phone VARCHAR(15));\n",
      "Generated: INSERT INTO attorneys (attorney_id, attorney_name, email, lawyer_phone) VALUES ('Oliver Martinez', 'oliver.martinez@lawfirm.com');\n",
      "BLEU: 0.5891\n",
      "ROUGE-L: 0.8000\n",
      "METEOR: 0.8102\n",
      "GLEU: 0.8333\n",
      "Flesch Reading Ease: -50.4391\n",
      "BERT: 0.9840\n",
      "CoSIM: 0.9758\n",
      "Repetition Rate: 0.0000\n",
      "Novelty: 0.2814\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0006\n",
      "\n",
      "Example 7:\n",
      "Actual: SELECT plant_name, SUM(co2_emission_per_ton_produced) AS total_co2_emission FROM co2_emissions GROUP BY plant_name ORDER BY total_co2_emission DESC LIMIT 2;\n",
      "SQL Prompt: Identify the top 2 plants with the highest CO2 emissions per ton produced.\n",
      "SQL Context: CREATE TABLE co2_emissions (id INT PRIMARY KEY, plant_name VARCHAR(255), chemical_name VARCHAR(255), co2_emission_per_ton_produced DECIMAL(5,2)); INSERT INTO co2_emissions (id, plant_name, chemical_name, co2_emission_per_ton_produced) VALUES (1, 'Plant A', 'Nitric Acid', 2.3); INSERT INTO co2_emissions (id, plant_name, chemical_name, co2_emission_per_ton_produced) VALUES (2, 'Plant B', 'Acetic Acid', 1.8);\n",
      "Generated: SELECT plant_name, SUM(co2_emission_per_ton_produced) as total_tons, MAX(total_years) AS total-to_non-exhausted_hours FROM co2 emissions GROUP BY id;\n",
      "BLEU: 0.3412\n",
      "ROUGE-L: 0.5714\n",
      "METEOR: 0.3480\n",
      "GLEU: 0.6486\n",
      "Flesch Reading Ease: -0.7464\n",
      "BERT: 0.9115\n",
      "CoSIM: 0.8293\n",
      "Repetition Rate: 0.0000\n",
      "Novelty: 0.2123\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0006\n",
      "\n",
      "Example 8:\n",
      "Actual: SELECT SUM(total_cost) FROM climate_communication WHERE location = 'Antarctica' AND end_date >= '2010-12-31';\n",
      "SQL Prompt: What is the total cost of all climate communication projects that ended after 2010 in 'Antarctica'?\n",
      "SQL Context: CREATE TABLE climate_communication (project_id INT, project_name VARCHAR(255), location VARCHAR(255), start_date DATE, end_date DATE, total_cost DECIMAL(10,2));\n",
      "Generated: SELECT SUM(total_cost) FROM climate_communication WHERE location = 'Antarctica' AND start_date >= DATE_SUB(CURRENT_DATE, INTERVAL 1 YEAR);\n",
      "BLEU: 0.5239\n",
      "ROUGE-L: 0.8421\n",
      "METEOR: 0.7437\n",
      "GLEU: 0.7692\n",
      "Flesch Reading Ease: -40.6369\n",
      "BERT: 0.9140\n",
      "CoSIM: 0.9395\n",
      "Repetition Rate: 0.0000\n",
      "Novelty: 0.1315\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0006\n",
      "\n",
      "Example 9:\n",
      "Actual: SELECT name, conservation_status FROM marine_species;\n",
      "SQL Prompt: List all marine species with their conservation status.\n",
      "SQL Context: CREATE TABLE marine_species (name TEXT, conservation_status TEXT);\n",
      "Generated: SELECT name FROM marine_species WHERE conservation_status = TRUE;\n",
      "BLEU: 0.0794\n",
      "ROUGE-L: 0.7143\n",
      "METEOR: 0.7099\n",
      "GLEU: 0.7500\n",
      "Flesch Reading Ease: 42.6157\n",
      "BERT: 0.9244\n",
      "CoSIM: 0.9340\n",
      "Repetition Rate: 0.0000\n",
      "Novelty: 0.1978\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0006\n",
      "\n",
      "Example 10:\n",
      "Actual: SELECT organization, AVG(publications) as avg_publications FROM researchers WHERE organization = 'National Geographic';\n",
      "SQL Prompt: What is the average number of publications per researcher for researchers working at the 'National Geographic' organization?\n",
      "SQL Context: CREATE TABLE researchers (id INT, name VARCHAR(255), organization VARCHAR(255), publications INT); INSERT INTO researchers (id, name, organization, publications) VALUES (1, 'Alice Johnson', 'NOAA', 25); INSERT INTO researchers (id, name, organization, publications) VALUES (2, 'Bob Smith', 'University of Miami', 30); INSERT INTO researchers (id, name, organization, publications) VALUES (3, 'Charlie Brown', 'National Geographic', 50);\n",
      "Generated: SELECT AVG(publications) FROM researchers WHERE organization = 'National Geographic';\n",
      "BLEU: 0.5991\n",
      "ROUGE-L: 0.7778\n",
      "METEOR: 0.7915\n",
      "GLEU: 0.9032\n",
      "Flesch Reading Ease: -55.0850\n",
      "BERT: 0.9641\n",
      "CoSIM: 0.9574\n",
      "Repetition Rate: 0.0000\n",
      "Novelty: 0.1972\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0006\n"
     ]
    }
   ],
   "source": [
    "# read the saved model under project folder and evaluate the performance on the test set\n",
    "\n",
    "# Load the base model and tokenizer\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "# Load the LoRA weights from checkpoint\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, \"./flan-t5-large-sql-lora-final\")\n",
    "finetuned_model.to(device)\n",
    "\n",
    "print(\"Model loaded successfully from checkpoint!\")\n",
    "\n",
    "# Run evaluation\n",
    "results, generated_sqls, reference_sqls, test_dataset = evaluate_sql_generation(finetuned_model, tokenizer, test_samples=50)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION RESULTS FOR T5 LARGE FINETUNED WITH LORA MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "metrics_summary = {}\n",
    "for metric_name, scores in results.items():\n",
    "    if scores:\n",
    "        mean_score = np.mean(scores)\n",
    "        std_score = np.std(scores)\n",
    "        metrics_summary[metric_name] = {\n",
    "            'mean': mean_score,\n",
    "            'std': std_score\n",
    "        }\n",
    "        print(f\"{metric_name.replace('_', ' ').title()}: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "\n",
    "# Print some examples\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE GENERATIONS\")\n",
    "print(\"=\"*50)\n",
    "for i in range(min(10, len(generated_sqls))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Actual: {test_dataset[i]['sql']}\")\n",
    "    print(f\"SQL Prompt: {test_dataset[i]['sql_prompt']}\")\n",
    "    print(f\"SQL Context: {test_dataset[i]['sql_context']}\")\n",
    "    print(f\"Generated: {generated_sqls[i]}\")\n",
    "    print(f\"BLEU: {results['bleu_scores'][i]:.4f}\")\n",
    "    print(f\"ROUGE-L: {results['rouge_l_scores'][i]:.4f}\")\n",
    "    print(f\"METEOR: {results['meteor_scores'][i]:.4f}\")\n",
    "    print(f\"GLEU: {results['gleu_scores'][i]:.4f}\")\n",
    "    print(f\"Flesch Reading Ease: {results['flesch_reading_ease_scores'][i]:.4f}\")\n",
    "    print(f\"BERT: {results['bert_scores'][i]:.4f}\")\n",
    "    print(f\"CoSIM: {results['cosim_scores'][i]:.4f}\")\n",
    "    print(f\"Repetition Rate: {results['repetition_rates'][i]:.4f}\")\n",
    "    print(f\"Novelty: {results['novelty_scores'][i]:.4f}\")\n",
    "    print(f\"Diversity: {results['diversity_scores'][i]:.4f}\")   \n",
    "    print(f\"Toxicity: {results['toxicity_scores'][i]:.4f}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from checkpoint!\n",
      "Running model on device type mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/1 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS FOR T5 LARGE FINETUNED WITH LORA MODEL\n",
      "==================================================\n",
      "Bleu Scores: 0.4775 ± 0.0000\n",
      "Rouge 1 Scores: 0.6667 ± 0.0000\n",
      "Rouge 2 Scores: 0.5000 ± 0.0000\n",
      "Rouge L Scores: 0.6667 ± 0.0000\n",
      "Meteor Scores: 0.6617 ± 0.0000\n",
      "Gleu Scores: 0.7059 ± 0.0000\n",
      "Flesch Reading Ease Scores: 54.7250 ± 0.0000\n",
      "Bert Scores: 0.9349 ± 0.0000\n",
      "Cosim Scores: 0.7254 ± 0.0000\n",
      "Repetition Rates: 0.0000 ± 0.0000\n",
      "Novelty Scores: 0.5000 ± 0.0000\n",
      "Diversity Scores: 1.0000 ± 0.0000\n",
      "Toxicity Scores: 0.0006 ± 0.0000\n",
      "\n",
      "==================================================\n",
      "SAMPLE GENERATIONS\n",
      "==================================================\n",
      "\n",
      "Example 1:\n",
      "Actual: SELECT COUNT(*) AS total_employees FROM employees\n",
      "SQL Prompt: What is the total number of employees in the company?\n",
      "SQL Context: \n",
      "Generated: SELECT COUNT(*) FROM company;\n",
      "BLEU: 0.4775\n",
      "ROUGE-L: 0.6667\n",
      "METEOR: 0.6617\n",
      "GLEU: 0.7059\n",
      "Flesch Reading Ease: 54.7250\n",
      "BERT: 0.9349\n",
      "CoSIM: 0.7254\n",
      "Repetition Rate: 0.0000\n",
      "Novelty: 0.5000\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0006\n"
     ]
    }
   ],
   "source": [
    "# This method takes an input text along with context (optional) and returns a SQL query.\n",
    "# It evaluates the SQL query using the evaluate_sql_generation function and prints the results.\n",
    "\n",
    "def evaluate_sample_sql_generation(model, tokenizer, sample_dataset):\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Running model on device type {device}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    results, generated_sqls, reference_sqls, test_dataset = evaluate_sql_generation(model, tokenizer, test_samples=0, sample_dataset=sample_dataset)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EVALUATION RESULTS FOR T5 LARGE FINETUNED WITH LORA MODEL\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    metrics_summary = {}\n",
    "    for metric_name, scores in results.items():\n",
    "        if scores:\n",
    "            mean_score = np.mean(scores)\n",
    "            std_score = np.std(scores)\n",
    "            metrics_summary[metric_name] = {\n",
    "                'mean': mean_score,\n",
    "                'std': std_score\n",
    "            }\n",
    "            print(f\"{metric_name.replace('_', ' ').title()}: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "\n",
    "    # Print some examples\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SAMPLE GENERATIONS\")\n",
    "    print(\"=\"*50)\n",
    "    for i in range(min(10, len(generated_sqls))):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Actual: {test_dataset[i]['sql']}\")\n",
    "        print(f\"SQL Prompt: {test_dataset[i]['sql_prompt']}\")\n",
    "        print(f\"SQL Context: {test_dataset[i]['sql_context']}\")\n",
    "        print(f\"Generated: {generated_sqls[i]}\")\n",
    "        print(f\"BLEU: {results['bleu_scores'][i]:.4f}\")\n",
    "        print(f\"ROUGE-L: {results['rouge_l_scores'][i]:.4f}\")\n",
    "        print(f\"METEOR: {results['meteor_scores'][i]:.4f}\")\n",
    "        print(f\"GLEU: {results['gleu_scores'][i]:.4f}\")\n",
    "        print(f\"Flesch Reading Ease: {results['flesch_reading_ease_scores'][i]:.4f}\")\n",
    "        print(f\"BERT: {results['bert_scores'][i]:.4f}\")\n",
    "        print(f\"CoSIM: {results['cosim_scores'][i]:.4f}\")\n",
    "        print(f\"Repetition Rate: {results['repetition_rates'][i]:.4f}\")\n",
    "        print(f\"Novelty: {results['novelty_scores'][i]:.4f}\")\n",
    "        print(f\"Diversity: {results['diversity_scores'][i]:.4f}\")   \n",
    "        print(f\"Toxicity: {results['toxicity_scores'][i]:.4f}\")\n",
    "\n",
    "# Load the base model and tokenizer\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "# Load the LoRA weights from checkpoint\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, \"./flan-t5-large-sql-lora-final\")\n",
    "finetuned_model.to(device)\n",
    "\n",
    "print(\"Model loaded successfully from checkpoint!\")  \n",
    "\n",
    "input_text = \"What is the total number of employees in the company?\"\n",
    "#context = \"CREATE TABLE employees (id INT, name VARCHAR(255), department VARCHAR(255), salary INT)\"\n",
    "context = \"\"\n",
    "reference_sql = \"SELECT COUNT(*) AS total_employees FROM employees\"\n",
    "\n",
    "# convert to dataset format\n",
    "sample_dataset = [{'sql_prompt': input_text, 'sql_context': context, 'sql': reference_sql}]\n",
    "\n",
    "evaluate_sample_sql_generation(finetuned_model, tokenizer, sample_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
