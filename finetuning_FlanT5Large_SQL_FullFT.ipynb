{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install peft nltk datasets sentencepiece 'accelerate>=0.26.0' \"transformers[torch]\" textstat sentence-transformers bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PyTorch with MPS backend\n",
    "import torch\n",
    "import random\n",
    "import nltk\n",
    "import textstat\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For Flan T5 Large\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import pipeline\n",
    "from collections import Counter\n",
    "from bert_score import score\n",
    "\n",
    "# Load the pretrained T5 model (not fine-tuned)\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load the dataset for evaluation\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"gretelai/synthetic_text_to_sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing pretrained model for Apple MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"device is {device}\")\n",
    "\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining method for calculating evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple ROUGE implementation without external dependencies\n",
    "def calculate_rouge_scores(reference, candidate):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE-1, ROUGE-2, ROUGE-L scores\n",
    "    \"\"\"\n",
    "    ref_tokens = set(nltk.word_tokenize(reference.lower()))\n",
    "    cand_tokens = set(nltk.word_tokenize(candidate.lower()))\n",
    "\n",
    "    # ROUGE-1\n",
    "    rouge1 = (\n",
    "        len(ref_tokens.intersection(cand_tokens)) / len(ref_tokens) if ref_tokens else 0\n",
    "    )\n",
    "\n",
    "    # ROUGE-2 (bigrams)\n",
    "    ref_bigrams = set(\n",
    "        zip(\n",
    "            nltk.word_tokenize(reference.lower())[:-1],\n",
    "            nltk.word_tokenize(reference.lower())[1:],\n",
    "        )\n",
    "    )\n",
    "    cand_bigrams = set(\n",
    "        zip(\n",
    "            nltk.word_tokenize(candidate.lower())[:-1],\n",
    "            nltk.word_tokenize(candidate.lower())[1:],\n",
    "        )\n",
    "    )\n",
    "    rouge2 = (\n",
    "        len(ref_bigrams.intersection(cand_bigrams)) / len(ref_bigrams)\n",
    "        if ref_bigrams\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    # ROUGE-L (longest common subsequence)\n",
    "    def lcs_length(s1, s2):\n",
    "        m, n = len(s1), len(s2)\n",
    "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "        for i in range(1, m + 1):\n",
    "            for j in range(1, n + 1):\n",
    "                if s1[i - 1] == s2[j - 1]:\n",
    "                    dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "                else:\n",
    "                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
    "        return dp[m][n]\n",
    "\n",
    "    ref_words = nltk.word_tokenize(reference.lower())\n",
    "    cand_words = nltk.word_tokenize(candidate.lower())\n",
    "    lcs = lcs_length(ref_words, cand_words)\n",
    "    rouge_l = lcs / len(ref_words) if ref_words else 0\n",
    "\n",
    "    return rouge1, rouge2, rouge_l\n",
    "\n",
    "\n",
    "# Calculate METEOR score\n",
    "def calculate_meteor(reference_sql, generated_sql):\n",
    "    ref_tokens = [word_tokenize(reference_sql.lower())]\n",
    "    gen_tokens = word_tokenize(generated_sql.lower())\n",
    "    score = meteor_score(ref_tokens, gen_tokens)\n",
    "    return score\n",
    "\n",
    "\n",
    "# Calculate GLEU score\n",
    "def calculate_gleu(reference_sql, generated_sql):\n",
    "    ref_tokens = word_tokenize(reference_sql.lower())\n",
    "    gen_tokens = word_tokenize(generated_sql.lower())\n",
    "\n",
    "    # Calculate n-gram overlaps\n",
    "    def get_ngrams(tokens, n):\n",
    "        return [tuple(tokens[i : i + n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "    # GLEU calculation (simplified version)\n",
    "    ref_1grams = set(get_ngrams(ref_tokens, 1))\n",
    "    gen_1grams = set(get_ngrams(gen_tokens, 1))\n",
    "\n",
    "    precision = (\n",
    "        len(ref_1grams.intersection(gen_1grams)) / len(gen_1grams) if gen_1grams else 0\n",
    "    )\n",
    "    recall = (\n",
    "        len(ref_1grams.intersection(gen_1grams)) / len(ref_1grams) if ref_1grams else 0\n",
    "    )\n",
    "\n",
    "    gleu = (\n",
    "        2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    )\n",
    "    return gleu\n",
    "\n",
    "\n",
    "# Calculate Fleash Reading Ease score which measures readability and complexity of the SQL\n",
    "def calculate_flesch_reading_ease(generated_sql):\n",
    "    # Convert SQL to more readable format for scoring\n",
    "    readable_sql = generated_sql.replace(\"SELECT\", \"SELECT \").replace(\"FROM\", \" FROM \")\n",
    "    flesch_score = textstat.flesch_reading_ease(readable_sql)\n",
    "    return flesch_score\n",
    "\n",
    "\n",
    "# Calculate BERT score\n",
    "def calculate_bert_score(reference_sql, generated_sql):\n",
    "    try:\n",
    "        bert_score = score([generated_sql], [reference_sql], lang=\"en\", verbose=False)\n",
    "        return bert_score[0].item()  # Return F1 scores\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BERT score for {generated_sql}: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Calculate CoSIM score\n",
    "def calculate_cosim_score(reference_sql, generated_sql):\n",
    "    sentence_transformer_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    reference_embeddings = sentence_transformer_model.encode(reference_sql)\n",
    "    generated_embeddings = sentence_transformer_model.encode(generated_sql)\n",
    "    # Calculate cosine similarity using dot product\n",
    "    ref_norm = reference_embeddings / np.linalg.norm(reference_embeddings)\n",
    "    gen_norm = generated_embeddings / np.linalg.norm(generated_embeddings)\n",
    "\n",
    "    cosim_score = np.dot(ref_norm, gen_norm)\n",
    "    return cosim_score\n",
    "\n",
    "\n",
    "def calculate_toxicity_score(generated_sql):\n",
    "    \"\"\"\n",
    "    Calculate toxicity score for a single SQL query\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize toxicity analyzer\n",
    "        toxicity_analyzer = pipeline(\n",
    "            \"text-classification\", model=\"unitary/toxic-bert\", return_all_scores=True\n",
    "        )\n",
    "\n",
    "        # Analyze toxicity of the SQL query\n",
    "        results = toxicity_analyzer(generated_sql)\n",
    "        # Get the maximum toxicity score across all categories\n",
    "        # Categories: toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
    "        max_toxicity = max([score[\"score\"] for score in results[0]])\n",
    "\n",
    "        return max_toxicity\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating toxicity score: {e}\")\n",
    "        # Return 0 if toxicity calculation fails\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# Evaluation function - REPLACE YOUR EXISTING FUNCTION WITH THIS\n",
    "def evaluate_sql_generation(model, tokenizer, test_samples=50, sample_dataset=None):\n",
    "    \"\"\"\n",
    "    Evaluate T5 Large pretrained model on SQL generation\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"bleu_scores\": [],\n",
    "        \"rouge_1_scores\": [],\n",
    "        \"rouge_2_scores\": [],\n",
    "        \"rouge_l_scores\": [],\n",
    "        \"meteor_scores\": [],\n",
    "        \"gleu_scores\": [],\n",
    "        \"flesch_reading_ease_scores\": [],\n",
    "        \"bert_scores\": [],\n",
    "        \"cosim_scores\": [],\n",
    "        \"repetition_rates\": [],\n",
    "        \"novelty_scores\": [],\n",
    "        \"diversity_scores\": [],\n",
    "        \"toxicity_scores\": [],\n",
    "    }\n",
    "\n",
    "    # Get test samples\n",
    "    if test_samples > 0:\n",
    "        test_data = dataset[\"test\"].select(\n",
    "            range(min(test_samples, len(dataset[\"test\"])))\n",
    "        )\n",
    "    else:\n",
    "        test_data = sample_dataset\n",
    "\n",
    "    generated_sqls = []\n",
    "    reference_sqls = []\n",
    "    prompts = []\n",
    "\n",
    "    for i, example in enumerate(test_data):\n",
    "        # Prepare input\n",
    "        input_text = (\n",
    "            f\"Question: {example['sql_prompt']} Context: {example['sql_context']}, IMPORTANT: With or without the context, your answer must always be a SQL statement\",\n",
    "        )\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            input_text, return_tensors=\"pt\", max_length=512, truncation=True\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate SQL\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=256,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2,\n",
    "            )\n",
    "\n",
    "        generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        reference_sql = example[\"sql\"]\n",
    "\n",
    "        generated_sqls.append(generated_sql)\n",
    "        reference_sqls.append(reference_sql)\n",
    "        prompts.append(example[\"sql_prompt\"])\n",
    "\n",
    "        # Calculate metrics\n",
    "        # 1. BLEU Score\n",
    "        reference_tokens = nltk.word_tokenize(reference_sql.lower())\n",
    "        generated_tokens = nltk.word_tokenize(generated_sql.lower())\n",
    "        bleu_score = sentence_bleu(\n",
    "            [reference_tokens],\n",
    "            generated_tokens,\n",
    "            smoothing_function=SmoothingFunction().method1,\n",
    "        )\n",
    "        results[\"bleu_scores\"].append(bleu_score)\n",
    "\n",
    "        # 2. ROUGE Scores\n",
    "        rouge1, rouge2, rouge_l = calculate_rouge_scores(reference_sql, generated_sql)\n",
    "        results[\"rouge_1_scores\"].append(rouge1)\n",
    "        results[\"rouge_2_scores\"].append(rouge2)\n",
    "        results[\"rouge_l_scores\"].append(rouge_l)\n",
    "\n",
    "        meteor_score = calculate_meteor(reference_sql, generated_sql)\n",
    "        results[\"meteor_scores\"].append(meteor_score)\n",
    "\n",
    "        # 3. GLEU Score\n",
    "        gleu_score = calculate_gleu(reference_sql, generated_sql)\n",
    "        results[\"gleu_scores\"].append(gleu_score)\n",
    "\n",
    "        # 4. Fleash Reading Ease Score\n",
    "        flesch_reading_ease_score = calculate_flesch_reading_ease(generated_sql)\n",
    "        results[\"flesch_reading_ease_scores\"].append(flesch_reading_ease_score)\n",
    "\n",
    "        # 5. BERT Score\n",
    "        bert_score = calculate_bert_score(reference_sql, generated_sql)\n",
    "        results[\"bert_scores\"].append(bert_score)\n",
    "\n",
    "        # 6. CoSIM Score\n",
    "        cosim_score = calculate_cosim_score(reference_sql, generated_sql)\n",
    "        results[\"cosim_scores\"].append(cosim_score)\n",
    "\n",
    "        # 3. Repetition Rate\n",
    "        # It is the percentage of tokens that are repeated in the generated SQL\n",
    "        tokens = generated_sql.split()\n",
    "        if len(tokens) > 0:\n",
    "            repetition_rate = 1 - len(set(tokens)) / len(tokens)\n",
    "        else:\n",
    "            repetition_rate = 0\n",
    "        results[\"repetition_rates\"].append(repetition_rate)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processed {i+1}/{len(test_data)} samples\")\n",
    "\n",
    "    # Calculate novelty and diversity across all generated SQLs\n",
    "    # To calculate novelty, we need to count the frequency of each token in the generated SQLs\n",
    "    # Then we can calculate the average novelty of the generated SQLs\n",
    "\n",
    "    all_tokens = []\n",
    "    for sql in generated_sqls:\n",
    "        all_tokens.extend(nltk.word_tokenize(sql.lower()))\n",
    "\n",
    "    # 4. Novelty (how different from common patterns)\n",
    "    token_freq = Counter(all_tokens)\n",
    "    novelty_scores = []\n",
    "    for sql in generated_sqls:\n",
    "        tokens = nltk.word_tokenize(sql.lower())\n",
    "        avg_novelty = np.mean([1 / (token_freq.get(token, 1) + 1) for token in tokens])\n",
    "        novelty_scores.append(avg_novelty)\n",
    "    results[\"novelty_scores\"] = novelty_scores\n",
    "\n",
    "    # 5. Diversity (unique SQL patterns)\n",
    "    unique_patterns = len(set(generated_sqls))\n",
    "    diversity_score = unique_patterns / len(generated_sqls)\n",
    "    results[\"diversity_scores\"] = [diversity_score] * len(generated_sqls)\n",
    "\n",
    "    # 6. Toxicity Score\n",
    "    toxicity_scores = [calculate_toxicity_score(sql) for sql in generated_sqls]\n",
    "    results[\"toxicity_scores\"] = toxicity_scores\n",
    "\n",
    "    return results, generated_sqls, reference_sqls, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intializing full fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 783,150,080\n",
      "Trainable parameters: 783,150,080\n",
      "Percentage of trainable parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "#### Full Fine-tuning Setup (No PEFT)\n",
    "\n",
    "# For full fine-tuning, we don't need PEFT configuration\n",
    "# The model will be fine-tuned with all parameters\n",
    "\n",
    "# Enable gradient checkpointing to save memory during full fine-tuning\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Print model information\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(\n",
    "    f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data:   0%|          | 0/1000 [00:00<?, ? examples/s]/Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3950: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Processing training data: 100%|██████████| 1000/1000 [00:00<00:00, 2635.26 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing validation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation data: 100%|██████████| 58/58 [00:00<00:00, 1838.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1000\n",
      "Validation dataset size: 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Preparing train and test datasets\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset for Text-to-SQL generation\n",
    "    \"\"\"\n",
    "    # Format input as instruction\n",
    "    inputs = [\n",
    "        f\"Question: {prompt} Context: {context}\"\n",
    "        for prompt, context in zip(examples[\"sql_prompt\"], examples[\"sql_context\"])\n",
    "    ]\n",
    "\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Tokenize targets (SQL queries)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"sql\"],\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # Replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "    model_inputs[\"labels\"] = torch.where(\n",
    "        model_inputs[\"labels\"] == tokenizer.pad_token_id, -100, model_inputs[\"labels\"]\n",
    "    )\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Updated data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "# Prepare datasets\n",
    "print(\"Preparing training dataset...\")\n",
    "train_indices = random.sample(\n",
    "    range(len(dataset[\"train\"])), int(0.01 * len(dataset[\"train\"]))\n",
    ")\n",
    "train_dataset = (\n",
    "    dataset[\"train\"]\n",
    "    .select(train_indices)\n",
    "    .map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        desc=\"Processing training data\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Preparing validation dataset...\")\n",
    "val_indices = random.sample(\n",
    "    range(len(dataset[\"test\"])), int(0.01 * len(dataset[\"test\"]))\n",
    ")\n",
    "val_dataset = (\n",
    "    dataset[\"test\"]\n",
    "    .select(val_indices)\n",
    "    .map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset[\"test\"].column_names,\n",
    "        desc=\"Processing validation data\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Debug dataset structure\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Training arguments with epochs and evaluation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./flan-t5-large-sql-fullft\",\n",
    "    num_train_epochs=1,  # Train for 3 epochs\n",
    "    eval_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",  # Save after each epoch\n",
    "    learning_rate=1e-5,  # Lower learning rate for full fine-tuning\n",
    "    per_device_train_batch_size=1,  # Smaller batch size for memory constraints\n",
    "    per_device_eval_batch_size=1,\n",
    "    # warmup_steps=50,\n",
    "    logging_steps=5,\n",
    "    save_total_limit=2,  # Keep last 3 checkpoints\n",
    "    # load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"eval_loss\",\n",
    "    # greater_is_better=False,\n",
    "    # dataloader_pin_memory=False,\n",
    "    # remove_unused_columns=False, # Removed this line\n",
    "    # Add evaluation metrics\n",
    "    # eval_steps=None,  # Remove this when using epoch strategy,\n",
    "    # Add these for speed\n",
    "    # dataloader_num_workers=0,       # Disable multiprocessing\n",
    "    report_to=None,\n",
    "    # Memory optimization for full fine-tuning\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=0,\n",
    "    # Add gradient accumulation for effective larger batch size\n",
    "    gradient_accumulation_steps=8,\n",
    "    # Add weight decay for regularization\n",
    "    weight_decay=0.01,\n",
    "    # fp16=True  # Enable mixed precision training\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Full fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  4/125 00:13 < 13:58, 0.14 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Start fine-tuning\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting Full fine-tuning...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Save the fully fine-tuned model\u001b[39;00m\n\u001b[32m      6\u001b[39m trainer.save_model(\u001b[33m\"\u001b[39m\u001b[33m./flan-t5-large-sql-fullft-final\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/transformers/trainer.py:2237\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2235\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2237\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2238\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/transformers/trainer.py:2578\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2571\u001b[39m context = (\n\u001b[32m   2572\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2573\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2574\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2575\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2576\u001b[39m )\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2578\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2581\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2582\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2583\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2584\u001b[39m ):\n\u001b[32m   2585\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2586\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/transformers/trainer.py:3840\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3837\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3838\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3840\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3842\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/accelerate/accelerator.py:2578\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2576\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2578\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Start fine-tuning\n",
    "print(\"Starting Full fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fully fine-tuned model\n",
    "trainer.save_model(\"./flan-t5-large-sql-fullft-final\")\n",
    "print(\"Full fine-tuning completed and model saved!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmarking finetuned model on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Model loaded successfully from checkpoint!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/10 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n",
      "/Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS FOR T5 LARGE FINETUNED WITH LORA MODEL\n",
      "==================================================\n",
      "Bleu Scores: 0.1132 ± 0.1208\n",
      "Rouge 1 Scores: 0.4688 ± 0.2088\n",
      "Rouge 2 Scores: 0.2393 ± 0.2645\n",
      "Rouge L Scores: 0.3780 ± 0.2179\n",
      "Meteor Scores: 0.3252 ± 0.2044\n",
      "Gleu Scores: 0.4883 ± 0.1776\n",
      "Flesch Reading Ease Scores: -7.0155 ± 47.8448\n",
      "Bert Scores: 0.8910 ± 0.0373\n",
      "Cosim Scores: 0.7334 ± 0.1178\n",
      "Repetition Rates: 0.0311 ± 0.0399\n",
      "Novelty Scores: 0.2574 ± 0.0332\n",
      "Diversity Scores: 1.0000 ± 0.0000\n",
      "Toxicity Scores: 0.0006 ± 0.0000\n",
      "\n",
      "==================================================\n",
      "SAMPLE GENERATIONS\n",
      "==================================================\n",
      "\n",
      "Example 1:\n",
      "Actual: SELECT AVG(explainability_score) FROM creative_ai WHERE region IN ('Europe', 'North America');\n",
      "SQL Prompt: What is the average explainability score of creative AI applications in 'Europe' and 'North America' in the 'creative_ai' table?\n",
      "SQL Context: CREATE TABLE creative_ai (application_id INT, name TEXT, region TEXT, explainability_score FLOAT); INSERT INTO creative_ai (application_id, name, region, explainability_score) VALUES (1, 'ApplicationX', 'Europe', 0.87), (2, 'ApplicationY', 'North America', 0.91), (3, 'ApplicationZ', 'Europe', 0.84), (4, 'ApplicationAA', 'North America', 0.93), (5, 'ApplicationAB', 'Europe', 0.89);\n",
      "Generated: SELECT Avg ( explainability_score ) FROM creative_ai\n",
      "BLEU: 0.1801\n",
      "ROUGE-L: 0.3684\n",
      "METEOR: 0.3216\n",
      "GLEU: 0.6087\n",
      "Flesch Reading Ease: -1.2800\n",
      "BERT: 0.9586\n",
      "CoSIM: 0.8160\n",
      "Repetition Rate: 0.0000\n",
      "Novelty: 0.2571\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0006\n",
      "\n",
      "Example 2:\n",
      "Actual: DELETE FROM rural_infrastructure WHERE country = 'Indonesia' AND completion_date < '2010-01-01';\n",
      "SQL Prompt: Delete all records of rural infrastructure projects in Indonesia that have a completion date before 2010.\n",
      "SQL Context: CREATE TABLE rural_infrastructure (id INT, project_name TEXT, sector TEXT, country TEXT, completion_date DATE); INSERT INTO rural_infrastructure (id, project_name, sector, country, completion_date) VALUES (1, 'Water Supply Expansion', 'Infrastructure', 'Indonesia', '2008-05-15'), (2, 'Rural Electrification', 'Infrastructure', 'Indonesia', '2012-08-28'), (3, 'Transportation Improvement', 'Infrastructure', 'Indonesia', '2009-12-31');\n",
      "Generated: CREATE TABLE rural_infrastructure (id, project_name, sector, country, completion_date)\n",
      "BLEU: 0.0188\n",
      "ROUGE-L: 0.2143\n",
      "METEOR: 0.1071\n",
      "GLEU: 0.2500\n",
      "Flesch Reading Ease: -33.9350\n",
      "BERT: 0.8594\n",
      "CoSIM: 0.5651\n",
      "Repetition Rate: 0.0000\n",
      "Novelty: 0.2252\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0006\n",
      "\n",
      "Example 3:\n",
      "Actual: SELECT launch_provider, COUNT(*) FROM Accidents GROUP BY launch_provider;\n",
      "SQL Prompt: How many accidents have been recorded for SpaceX and Blue Origin rocket launches?\n",
      "SQL Context: CREATE TABLE Accidents (id INT, launch_provider VARCHAR(255), year INT, description TEXT); INSERT INTO Accidents (id, launch_provider, year, description) VALUES (1, 'SpaceX', 2015, 'Falcon 9 explosion'), (2, 'Blue Origin', 2011, 'Propulsion system failure'), (3, 'SpaceX', 2016, 'Falcon 9 explosion');\n",
      "Generated: SELECT count(*) FROM Accidents (id, launch_provider, year, description)\n",
      "BLEU: 0.3490\n",
      "ROUGE-L: 0.6154\n",
      "METEOR: 0.4391\n",
      "GLEU: 0.7500\n",
      "Flesch Reading Ease: 61.2400\n",
      "BERT: 0.8926\n",
      "CoSIM: 0.8712\n",
      "Repetition Rate: 0.0000\n",
      "Novelty: 0.2453\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0006\n",
      "\n",
      "Example 4:\n",
      "Actual: SELECT MAX(quantity) FROM sales;\n",
      "SQL Prompt: What is the maximum quantity of seafood sold in a single transaction?\n",
      "SQL Context: CREATE TABLE sales (id INT, location VARCHAR(20), quantity INT, price DECIMAL(5,2)); INSERT INTO sales (id, location, quantity, price) VALUES (1, 'Northeast', 50, 12.99), (2, 'Midwest', 75, 19.99), (3, 'West', 120, 14.49);\n",
      "Generated: CREATE TABLE sales (id, location, quantity, price) INTERSECT SELECT max(quantity) FROM sales\n",
      "BLEU: 0.2915\n",
      "ROUGE-L: 0.8750\n",
      "METEOR: 0.7598\n",
      "GLEU: 0.6364\n",
      "Flesch Reading Ease: 32.5050\n",
      "BERT: 0.8539\n",
      "CoSIM: 0.8041\n",
      "Repetition Rate: 0.0833\n",
      "Novelty: 0.2284\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0006\n",
      "\n",
      "Example 5:\n",
      "Actual: SELECT SUM(budget) FROM Movies_Release_Year WHERE release_year < 2010;\n",
      "SQL Prompt: What is the total budget for movies released before 2010?\n",
      "SQL Context: CREATE TABLE Movies_Release_Year (id INT, title VARCHAR(100), release_year INT, budget DECIMAL(10,2)); INSERT INTO Movies_Release_Year (id, title, release_year, budget) VALUES (1, 'The Matrix', 1999, 63000000.00), (2, 'Titanic', 1997, 200000000.00), (3, 'The Lord of the Rings: The Fellowship of the Ring', 2001, 93000000.00);\n",
      "Generated: SELECT budget FROM Movies_Release_Year\n",
      "BLEU: 0.0273\n",
      "ROUGE-L: 0.3333\n",
      "METEOR: 0.2818\n",
      "GLEU: 0.5000\n",
      "Flesch Reading Ease: 54.7250\n",
      "BERT: 0.9494\n",
      "CoSIM: 0.8420\n",
      "Repetition Rate: 0.0000\n",
      "Novelty: 0.3333\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0007\n",
      "\n",
      "Example 6:\n",
      "Actual: INSERT INTO attorneys (attorney_name, attorney_email, attorney_phone) VALUES ('Oliver Martinez', 'oliver.martinez@lawfirm.com', '555-555-5678');\n",
      "SQL Prompt: Add a new attorney named 'Oliver Martinez' with email 'oliver.martinez@lawfirm.com' and phone '555-555-5678' to the 'attorneys' table\n",
      "SQL Context: CREATE TABLE attorneys (attorney_id INT, attorney_name VARCHAR(50), attorney_email VARCHAR(50), attorney_phone VARCHAR(15));\n",
      "Generated: CREATE TABLE attorneys (attorney_id = INT, attorney_name = \"Oliver Martinez\" )\n",
      "BLEU: 0.0189\n",
      "ROUGE-L: 0.2000\n",
      "METEOR: 0.1250\n",
      "GLEU: 0.3750\n",
      "Flesch Reading Ease: -23.3600\n",
      "BERT: 0.8918\n",
      "CoSIM: 0.6839\n",
      "Repetition Rate: 0.0909\n",
      "Novelty: 0.2743\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0006\n",
      "\n",
      "Example 7:\n",
      "Actual: SELECT plant_name, SUM(co2_emission_per_ton_produced) AS total_co2_emission FROM co2_emissions GROUP BY plant_name ORDER BY total_co2_emission DESC LIMIT 2;\n",
      "SQL Prompt: Identify the top 2 plants with the highest CO2 emissions per ton produced.\n",
      "SQL Context: CREATE TABLE co2_emissions (id INT PRIMARY KEY, plant_name VARCHAR(255), chemical_name VARCHAR(255), co2_emission_per_ton_produced DECIMAL(5,2)); INSERT INTO co2_emissions (id, plant_name, chemical_name, co2_emission_per_ton_produced) VALUES (1, 'Plant A', 'Nitric Acid', 2.3); INSERT INTO co2_emissions (id, plant_name, chemical_name, co2_emission_per_ton_produced) VALUES (2, 'Plant B', 'Acetic Acid', 1.8);\n",
      "Generated: CREATE TABLE co2_emissions (id = t1.plant_name; plant_number = 2); INSERT INTO (t2.co2emittance_per_ton_produced) VALUES ('Plant A', 'Nitric Acid'; 2.3; Plant B, \"Acetic Acid\");\n",
      "BLEU: 0.0074\n",
      "ROUGE-L: 0.1429\n",
      "METEOR: 0.1316\n",
      "GLEU: 0.2667\n",
      "Flesch Reading Ease: 28.3300\n",
      "BERT: 0.8428\n",
      "CoSIM: 0.6729\n",
      "Repetition Rate: 0.0455\n",
      "Novelty: 0.2901\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0007\n",
      "\n",
      "Example 8:\n",
      "Actual: SELECT SUM(total_cost) FROM climate_communication WHERE location = 'Antarctica' AND end_date >= '2010-12-31';\n",
      "SQL Prompt: What is the total cost of all climate communication projects that ended after 2010 in 'Antarctica'?\n",
      "SQL Context: CREATE TABLE climate_communication (project_id INT, project_name VARCHAR(255), location VARCHAR(255), start_date DATE, end_date DATE, total_cost DECIMAL(10,2));\n",
      "Generated: CREATE TABLE climate_communication (project_id = INT, project_name = \"climate communication\" )\n",
      "BLEU: 0.0151\n",
      "ROUGE-L: 0.1579\n",
      "METEOR: 0.1344\n",
      "GLEU: 0.2581\n",
      "Flesch Reading Ease: -76.2350\n",
      "BERT: 0.8602\n",
      "CoSIM: 0.5003\n",
      "Repetition Rate: 0.0909\n",
      "Novelty: 0.2632\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0006\n",
      "\n",
      "Example 9:\n",
      "Actual: SELECT name, conservation_status FROM marine_species;\n",
      "SQL Prompt: List all marine species with their conservation status.\n",
      "SQL Context: CREATE TABLE marine_species (name TEXT, conservation_status TEXT);\n",
      "Generated: CREATE marine_species(name, conservation_status)\n",
      "BLEU: 0.1757\n",
      "ROUGE-L: 0.4286\n",
      "METEOR: 0.5357\n",
      "GLEU: 0.5714\n",
      "Flesch Reading Ease: -78.2100\n",
      "BERT: 0.9086\n",
      "CoSIM: 0.7648\n",
      "Repetition Rate: 0.0000\n",
      "Novelty: 0.2375\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0007\n",
      "\n",
      "Example 10:\n",
      "Actual: SELECT organization, AVG(publications) as avg_publications FROM researchers WHERE organization = 'National Geographic';\n",
      "SQL Prompt: What is the average number of publications per researcher for researchers working at the 'National Geographic' organization?\n",
      "SQL Context: CREATE TABLE researchers (id INT, name VARCHAR(255), organization VARCHAR(255), publications INT); INSERT INTO researchers (id, name, organization, publications) VALUES (1, 'Alice Johnson', 'NOAA', 25); INSERT INTO researchers (id, name, organization, publications) VALUES (2, 'Bob Smith', 'University of Miami', 30); INSERT INTO researchers (id, name, organization, publications) VALUES (3, 'Charlie Brown', 'National Geographic', 50);\n",
      "Generated: SELECT avg(number_of_publications) FROM researchers (id, name, organization, publications);\n",
      "BLEU: 0.0485\n",
      "ROUGE-L: 0.4444\n",
      "METEOR: 0.4156\n",
      "GLEU: 0.6667\n",
      "Flesch Reading Ease: -33.9350\n",
      "BERT: 0.8927\n",
      "CoSIM: 0.8140\n",
      "Repetition Rate: 0.0000\n",
      "Novelty: 0.2197\n",
      "Diversity: 1.0000\n",
      "Toxicity: 0.0006\n"
     ]
    }
   ],
   "source": [
    "# read the saved model under project folder and evaluate the performance on the test set\n",
    "    \n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load the base model and tokenizer\n",
    "fulltuned_model = T5ForConditionalGeneration.from_pretrained(\"./flan-t5-large-sql-fullft-final\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "fulltuned_model.to(device)\n",
    "\n",
    "print(\"Model loaded successfully from checkpoint!\")\n",
    "    \n",
    "# Run evaluation\n",
    "results, generated_sqls, reference_sqls, test_dataset = evaluate_sql_generation(fulltuned_model, tokenizer, test_samples=10)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION RESULTS FOR T5 LARGE FINETUNED WITH LORA MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "metrics_summary = {}\n",
    "for metric_name, scores in results.items():\n",
    "        if scores:\n",
    "            mean_score = np.mean(scores)\n",
    "            std_score = np.std(scores)\n",
    "            metrics_summary[metric_name] = {\n",
    "                'mean': mean_score,\n",
    "                'std': std_score\n",
    "            }\n",
    "            print(f\"{metric_name.replace('_', ' ').title()}: {mean_score:.4f} ± {std_score:.4f}\")    \n",
    "\n",
    "# Print some examples\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE GENERATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i in range(min(10, len(generated_sqls))):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Actual: {test_dataset[i]['sql']}\")\n",
    "        print(f\"SQL Prompt: {test_dataset[i]['sql_prompt']}\")\n",
    "        print(f\"SQL Context: {test_dataset[i]['sql_context']}\")\n",
    "        print(f\"Generated: {generated_sqls[i]}\")\n",
    "        print(f\"BLEU: {results['bleu_scores'][i]:.4f}\")\n",
    "        print(f\"ROUGE-L: {results['rouge_l_scores'][i]:.4f}\")\n",
    "        print(f\"METEOR: {results['meteor_scores'][i]:.4f}\")\n",
    "        print(f\"GLEU: {results['gleu_scores'][i]:.4f}\")\n",
    "        print(f\"Flesch Reading Ease: {results['flesch_reading_ease_scores'][i]:.4f}\")\n",
    "        print(f\"BERT: {results['bert_scores'][i]:.4f}\")\n",
    "        print(f\"CoSIM: {results['cosim_scores'][i]:.4f}\")\n",
    "        print(f\"Repetition Rate: {results['repetition_rates'][i]:.4f}\")\n",
    "        print(f\"Novelty: {results['novelty_scores'][i]:.4f}\")\n",
    "        print(f\"Diversity: {results['diversity_scores'][i]:.4f}\")   \n",
    "        print(f\"Toxicity: {results['toxicity_scores'][i]:.4f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method takes an input text along with context (optional) and returns a SQL query.\n",
    "# It evaluates the SQL query using the evaluate_sql_generation function and prints the results.\n",
    "\n",
    "\n",
    "def evaluate_sample_sql_generation(model, tokenizer, sample_dataset):\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Running model on device type {device}\")\n",
    "\n",
    "    # Run evaluation\n",
    "    results, generated_sqls, reference_sqls, test_dataset = evaluate_sql_generation(\n",
    "        model, tokenizer, test_samples=0, sample_dataset=sample_dataset\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"EVALUATION RESULTS FOR T5 LARGE FINETUNED WITH LORA MODEL\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    metrics_summary = {}\n",
    "    for metric_name, scores in results.items():\n",
    "        if scores:\n",
    "            mean_score = np.mean(scores)\n",
    "            std_score = np.std(scores)\n",
    "            metrics_summary[metric_name] = {\"mean\": mean_score, \"std\": std_score}\n",
    "            print(\n",
    "                f\"{metric_name.replace('_', ' ').title()}: {mean_score:.4f} ± {std_score:.4f}\"\n",
    "            )\n",
    "\n",
    "    # Print some examples\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"SAMPLE GENERATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    for i in range(min(10, len(generated_sqls))):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Actual: {test_dataset[i]['sql']}\")\n",
    "        print(f\"SQL Prompt: {test_dataset[i]['sql_prompt']}\")\n",
    "        print(f\"SQL Context: {test_dataset[i]['sql_context']}\")\n",
    "        print(f\"Generated: {generated_sqls[i]}\")\n",
    "        print(f\"BLEU: {results['bleu_scores'][i]:.4f}\")\n",
    "        print(f\"ROUGE-L: {results['rouge_l_scores'][i]:.4f}\")\n",
    "        print(f\"METEOR: {results['meteor_scores'][i]:.4f}\")\n",
    "        print(f\"GLEU: {results['gleu_scores'][i]:.4f}\")\n",
    "        print(f\"Flesch Reading Ease: {results['flesch_reading_ease_scores'][i]:.4f}\")\n",
    "        print(f\"BERT: {results['bert_scores'][i]:.4f}\")\n",
    "        print(f\"CoSIM: {results['cosim_scores'][i]:.4f}\")\n",
    "        print(f\"Repetition Rate: {results['repetition_rates'][i]:.4f}\")\n",
    "        print(f\"Novelty: {results['novelty_scores'][i]:.4f}\")\n",
    "        print(f\"Diversity: {results['diversity_scores'][i]:.4f}\")\n",
    "        print(f\"Toxicity: {results['toxicity_scores'][i]:.4f}\")\n",
    "\n",
    "\n",
    "# Load the base model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "fulltuned_model = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"./flan-t5-large-sql-fullft-final\"\n",
    ")\n",
    "fulltuned_model.to(device)\n",
    "print(\"Model loaded successfully from checkpoint!\")\n",
    "\n",
    "input_text = \"What is the total number of employees in the company?\"\n",
    "context = \"CREATE TABLE employees (id INT, name VARCHAR(255), department VARCHAR(255), salary INT)\"\n",
    "# context = \"\"\n",
    "reference_sql = \"SELECT COUNT(*) AS total_employees FROM employees\"\n",
    "\n",
    "# convert to dataset format\n",
    "sample_dataset = [\n",
    "    {\"sql_prompt\": input_text, \"sql_context\": context, \"sql\": reference_sql}\n",
    "]\n",
    "\n",
    "evaluate_sample_sql_generation(fulltuned_model, tokenizer, sample_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
