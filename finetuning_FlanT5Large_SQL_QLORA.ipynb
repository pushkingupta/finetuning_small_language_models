{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (0.17.0)\n",
      "Requirement already satisfied: nltk in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: datasets in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (4.0.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (1.9.0)\n",
      "Requirement already satisfied: transformers[torch] in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (4.54.1)\n",
      "Requirement already satisfied: bitsandbytes in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (0.42.0)\n",
      "Requirement already satisfied: textstat in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (0.7.8)\n",
      "Requirement already satisfied: sentence-transformers in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from peft) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from peft) (2.7.1)\n",
      "Requirement already satisfied: tqdm in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from peft) (0.34.3)\n",
      "Requirement already satisfied: click in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from nltk) (8.2.2)\n",
      "Requirement already satisfied: joblib in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: filelock in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from datasets) (2025.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from transformers[torch]) (0.21.4)\n",
      "Requirement already satisfied: scipy in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from bitsandbytes) (1.15.2)\n",
      "Requirement already satisfied: pyphen in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from textstat) (0.17.2)\n",
      "Requirement already satisfied: cmudict in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from textstat) (1.1.1)\n",
      "Requirement already satisfied: setuptools in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from textstat) (65.5.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: Pillow in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: importlib-metadata>=5 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from cmudict->textstat) (8.7.0)\n",
      "Requirement already satisfied: importlib-resources>=5 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from cmudict->textstat) (6.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from importlib-metadata>=5->cmudict->textstat) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install peft nltk datasets sentencepiece 'accelerate>=0.26.0' \"transformers[torch]\" bitsandbytes textstat sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pushking/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/pushking/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/pushking/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pushking/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Use PyTorch with MPS backend\n",
    "import torch\n",
    "import random\n",
    "import nltk\n",
    "import textstat\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For Flan T5 Large\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import pipeline\n",
    "from collections import Counter\n",
    "from bert_score import score\n",
    "# Load the pretrained T5 model (not fine-tuned)\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, BitsAndBytesConfig\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "# Loading PEFT configs\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset for evaluation\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gretelai/synthetic_text_to_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Initializing pretrained model for CUDA T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on mps device\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 4-bit quantization config\u001b[39;00m\n\u001b[32m      5\u001b[39m bnb_config = BitsAndBytesConfig(\n\u001b[32m      6\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      7\u001b[39m     bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     bnb_4bit_compute_dtype=torch.float16,\n\u001b[32m      9\u001b[39m     bnb_4bit_use_double_quant=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m model = \u001b[43mT5ForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle/flan-t5-large\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m tokenizer = T5Tokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mgoogle/flan-t5-large\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m model.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:315\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    317\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4822\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4819\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4822\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4823\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4827\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4828\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4829\u001b[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001b[32m   4830\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects@Adobe/StudyMaterial/IITGenAIDocs/Code/venv/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:88\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version.parse(importlib.metadata.version(\u001b[33m\"\u001b[39m\u001b[33mbitsandbytes\u001b[39m\u001b[33m\"\u001b[39m)) < version.parse(\u001b[33m\"\u001b[39m\u001b[33m0.43.1\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.cuda.is_available():\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     89\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     91\u001b[39m         )\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[31mImportError\u001b[39m: The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Training model on {device} device\")\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\",quantization_config=bnb_config)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining method for calculating evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple ROUGE implementation without external dependencies\n",
    "def calculate_rouge_scores(reference, candidate):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE-1, ROUGE-2, ROUGE-L scores\n",
    "    \"\"\"\n",
    "    ref_tokens = set(nltk.word_tokenize(reference.lower()))\n",
    "    cand_tokens = set(nltk.word_tokenize(candidate.lower()))\n",
    "    \n",
    "    # ROUGE-1\n",
    "    rouge1 = len(ref_tokens.intersection(cand_tokens)) / len(ref_tokens) if ref_tokens else 0\n",
    "    \n",
    "    # ROUGE-2 (bigrams)\n",
    "    ref_bigrams = set(zip(nltk.word_tokenize(reference.lower())[:-1], \n",
    "                          nltk.word_tokenize(reference.lower())[1:]))\n",
    "    cand_bigrams = set(zip(nltk.word_tokenize(candidate.lower())[:-1], \n",
    "                           nltk.word_tokenize(candidate.lower())[1:]))\n",
    "    rouge2 = len(ref_bigrams.intersection(cand_bigrams)) / len(ref_bigrams) if ref_bigrams else 0\n",
    "    \n",
    "    # ROUGE-L (longest common subsequence)\n",
    "    def lcs_length(s1, s2):\n",
    "        m, n = len(s1), len(s2)\n",
    "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "        for i in range(1, m + 1):\n",
    "            for j in range(1, n + 1):\n",
    "                if s1[i-1] == s2[j-1]:\n",
    "                    dp[i][j] = dp[i-1][j-1] + 1\n",
    "                else:\n",
    "                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "        return dp[m][n]\n",
    "    \n",
    "    ref_words = nltk.word_tokenize(reference.lower())\n",
    "    cand_words = nltk.word_tokenize(candidate.lower())\n",
    "    lcs = lcs_length(ref_words, cand_words)\n",
    "    rouge_l = lcs / len(ref_words) if ref_words else 0\n",
    "    \n",
    "    return rouge1, rouge2, rouge_l\n",
    "\n",
    "# Calculate METEOR score\n",
    "def calculate_meteor(reference_sql, generated_sql):\n",
    "    ref_tokens = [word_tokenize(reference_sql.lower())]\n",
    "    gen_tokens = word_tokenize(generated_sql.lower())\n",
    "    score = meteor_score(ref_tokens, gen_tokens)\n",
    "    return score\n",
    "\n",
    "# Calculate GLEU score\n",
    "def calculate_gleu(reference_sql, generated_sql):\n",
    "    ref_tokens = word_tokenize(reference_sql.lower())\n",
    "    gen_tokens = word_tokenize(generated_sql.lower())\n",
    "        \n",
    "    # Calculate n-gram overlaps\n",
    "    def get_ngrams(tokens, n):\n",
    "        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "        \n",
    "    # GLEU calculation (simplified version)\n",
    "    ref_1grams = set(get_ngrams(ref_tokens, 1))\n",
    "    gen_1grams = set(get_ngrams(gen_tokens, 1))\n",
    "        \n",
    "    precision = len(ref_1grams.intersection(gen_1grams)) / len(gen_1grams) if gen_1grams else 0\n",
    "    recall = len(ref_1grams.intersection(gen_1grams)) / len(ref_1grams) if ref_1grams else 0\n",
    "        \n",
    "    gleu = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return gleu\n",
    "\n",
    "# Calculate Fleash Reading Ease score which measures readability and complexity of the SQL\n",
    "def calculate_flesch_reading_ease(generated_sql):\n",
    "    # Convert SQL to more readable format for scoring\n",
    "    readable_sql = generated_sql.replace('SELECT', 'SELECT ').replace('FROM', ' FROM ')\n",
    "    flesch_score = textstat.flesch_reading_ease(readable_sql)\n",
    "    return flesch_score\n",
    "\n",
    "# Calculate BERT score\n",
    "def calculate_bert_score(reference_sql, generated_sql):\n",
    "    try: \n",
    "        bert_score = score([generated_sql], [reference_sql], lang='en', verbose=False)\n",
    "        return bert_score[0].item()   # Return F1 scores\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BERT score for {generated_sql}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Calculate CoSIM score\n",
    "def calculate_cosim_score(reference_sql, generated_sql):\n",
    "    sentence_transformer_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    reference_embeddings = sentence_transformer_model.encode(reference_sql)\n",
    "    generated_embeddings = sentence_transformer_model.encode(generated_sql)\n",
    "    # Calculate cosine similarity using dot product\n",
    "    ref_norm = reference_embeddings / np.linalg.norm(reference_embeddings)\n",
    "    gen_norm = generated_embeddings / np.linalg.norm(generated_embeddings)\n",
    "    \n",
    "    cosim_score = np.dot(ref_norm, gen_norm)\n",
    "    return cosim_score\n",
    "\n",
    "def calculate_toxicity_score(generated_sql):\n",
    "    \"\"\"\n",
    "    Calculate toxicity score for a single SQL query\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize toxicity analyzer\n",
    "        toxicity_analyzer = pipeline(\n",
    "            \"text-classification\", \n",
    "            model=\"unitary/toxic-bert\", \n",
    "            return_all_scores=True\n",
    "        )\n",
    "        \n",
    "        # Analyze toxicity of the SQL query\n",
    "        results = toxicity_analyzer(generated_sql)\n",
    "        # Get the maximum toxicity score across all categories\n",
    "        # Categories: toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
    "        max_toxicity = max([score['score'] for score in results[0]])\n",
    "        \n",
    "        return max_toxicity\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating toxicity score: {e}\")\n",
    "        # Return 0 if toxicity calculation fails\n",
    "        return 0.0\n",
    "\n",
    "# Evaluation function - REPLACE YOUR EXISTING FUNCTION WITH THIS\n",
    "def evaluate_sql_generation(model, tokenizer, test_samples=50):\n",
    "    \"\"\"\n",
    "    Evaluate T5 Large pretrained model on SQL generation\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'bleu_scores': [],\n",
    "        'rouge_1_scores': [],\n",
    "        'rouge_2_scores': [],\n",
    "        'rouge_l_scores': [],\n",
    "        'meteor_scores': [],\n",
    "        'gleu_scores': [],\n",
    "        'flesch_reading_ease_scores': [],  \n",
    "        'bert_scores': [],\n",
    "        'cosim_scores': [],\n",
    "        'repetition_rates': [],\n",
    "        'novelty_scores': [],\n",
    "        'diversity_scores': [],\n",
    "        'toxicity_scores': []\n",
    "    }\n",
    "    \n",
    "    # Get test samples\n",
    "    test_data = dataset['test'].select(range(min(test_samples, len(dataset['test']))))\n",
    "    \n",
    "    generated_sqls = []\n",
    "    reference_sqls = []\n",
    "    prompts = []\n",
    "    \n",
    "    for i, example in enumerate(test_data):\n",
    "        # Prepare input\n",
    "        input_text = f\"Question: {example['sql_prompt']} Context: {example['sql_context']}\"\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate SQL\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=256,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "        \n",
    "        generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        reference_sql = example['sql']\n",
    "        \n",
    "        generated_sqls.append(generated_sql)\n",
    "        reference_sqls.append(reference_sql)\n",
    "        prompts.append(example['sql_prompt'])\n",
    "    \n",
    "        \n",
    "        # Calculate metrics\n",
    "        # 1. BLEU Score\n",
    "        reference_tokens = nltk.word_tokenize(reference_sql.lower())\n",
    "        generated_tokens = nltk.word_tokenize(generated_sql.lower())\n",
    "        bleu_score = sentence_bleu([reference_tokens], generated_tokens, \n",
    "                                 smoothing_function=SmoothingFunction().method1)\n",
    "        results['bleu_scores'].append(bleu_score)\n",
    "        \n",
    "        # 2. ROUGE Scores\n",
    "        rouge1, rouge2, rouge_l = calculate_rouge_scores(reference_sql, generated_sql)\n",
    "        results['rouge_1_scores'].append(rouge1)\n",
    "        results['rouge_2_scores'].append(rouge2)\n",
    "        results['rouge_l_scores'].append(rouge_l)\n",
    "\n",
    "        meteor_score = calculate_meteor(reference_sql, generated_sql)\n",
    "        results['meteor_scores'].append(meteor_score)\n",
    "\n",
    "        # 3. GLEU Score\n",
    "        gleu_score = calculate_gleu(reference_sql, generated_sql)\n",
    "        results['gleu_scores'].append(gleu_score)\n",
    "\n",
    "        # 4. Fleash Reading Ease Score\n",
    "        flesch_reading_ease_score = calculate_flesch_reading_ease(generated_sql)\n",
    "        results['flesch_reading_ease_scores'].append(flesch_reading_ease_score)\n",
    "\n",
    "        # 5. BERT Score\n",
    "        bert_score = calculate_bert_score(reference_sql, generated_sql)\n",
    "        results['bert_scores'].append(bert_score)\n",
    "\n",
    "        # 6. CoSIM Score\n",
    "        cosim_score = calculate_cosim_score(reference_sql, generated_sql)\n",
    "        results['cosim_scores'].append(cosim_score)\n",
    "        \n",
    "        # 3. Repetition Rate\n",
    "        # It is the percentage of tokens that are repeated in the generated SQL\n",
    "        tokens = generated_sql.split()\n",
    "        if len(tokens) > 0:\n",
    "            repetition_rate = 1 - len(set(tokens)) / len(tokens)\n",
    "        else:\n",
    "            repetition_rate = 0\n",
    "        results['repetition_rates'].append(repetition_rate)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processed {i+1}/{len(test_data)} samples\")\n",
    "    \n",
    "    # Calculate novelty and diversity across all generated SQLs\n",
    "    # To calculate novelty, we need to count the frequency of each token in the generated SQLs\n",
    "    # Then we can calculate the average novelty of the generated SQLs\n",
    "\n",
    "    all_tokens = []\n",
    "    for sql in generated_sqls:\n",
    "        all_tokens.extend(nltk.word_tokenize(sql.lower()))\n",
    "    \n",
    "    # 4. Novelty (how different from common patterns)\n",
    "    token_freq = Counter(all_tokens)\n",
    "    novelty_scores = []\n",
    "    for sql in generated_sqls:\n",
    "        tokens = nltk.word_tokenize(sql.lower())\n",
    "        avg_novelty = np.mean([1 / (token_freq.get(token, 1) + 1) for token in tokens])\n",
    "        novelty_scores.append(avg_novelty)\n",
    "    results['novelty_scores'] = novelty_scores\n",
    "    \n",
    "    # 5. Diversity (unique SQL patterns)\n",
    "    unique_patterns = len(set(generated_sqls))\n",
    "    diversity_score = unique_patterns / len(generated_sqls)\n",
    "    results['diversity_scores'] = [diversity_score] * len(generated_sqls)\n",
    "\n",
    "    # 6. Toxicity Score\n",
    "    toxicity_scores = [calculate_toxicity_score(sql) for sql in generated_sqls]\n",
    "    results['toxicity_scores'] = toxicity_scores\n",
    "    \n",
    "    return results, generated_sqls, reference_sqls, test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Initializing PEFT using QLORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,                    # Rank of low-rank matrices\n",
    "    lora_alpha=16,           # Scaling factor\n",
    "    target_modules=[\"q\", \"v\"],  # Target all attention components\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM #Seq2SeqLM for T5\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.to(device)\n",
    "print(f\"Number of Trainable parameters: {model.print_trainable_parameters()}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing train and test datasets\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset for Text-to-SQL generation\n",
    "    \"\"\"\n",
    "    # Format input as instruction\n",
    "    inputs = [f\"Question: {prompt} Context: {context}\" \n",
    "                 for prompt, context in zip(examples['sql_prompt'], examples['sql_context'])]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets (SQL queries)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples['sql'],\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    # Replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "    model_inputs[\"labels\"] = torch.where(\n",
    "        model_inputs[\"labels\"] == tokenizer.pad_token_id,\n",
    "        -100,\n",
    "        model_inputs[\"labels\"]\n",
    "    )\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Updated data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "# Prepare datasets\n",
    "print(\"Preparing training dataset...\")\n",
    "train_indices = random.sample(range(len(dataset['train'])), int(0.2 * len(dataset['train'])))\n",
    "train_dataset = dataset['train'].select(train_indices).map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    "    desc=\"Processing training data\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Preparing validation dataset...\")\n",
    "val_indices = random.sample(range(len(dataset['test'])), int(0.2 * len(dataset['test'])))\n",
    "val_dataset = dataset['test'].select(val_indices).map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['test'].column_names,\n",
    "    desc=\"Processing validation data\"\n",
    ")\n",
    "\n",
    "# Debug dataset structure\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Check a sample\n",
    "sample = train_dataset[0]\n",
    "print(\"\\nSample from training dataset:\")\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {value.shape} - {value.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value)}\")\n",
    "\n",
    "# Training arguments with epochs and evaluation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./flan-t5-large-sql-qlora\",\n",
    "    num_train_epochs=1,  # Train for 3 epochs\n",
    "    eval_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",  # Save after each epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2, # Further reduced batch size\n",
    "    per_device_eval_batch_size=2,\n",
    "    #warmup_steps=50,\n",
    "    #logging_steps=5,\n",
    "    save_total_limit=2,  # Keep last 3 checkpoints\n",
    "    #load_best_model_at_end=True,\n",
    "    #metric_for_best_model=\"eval_loss\",\n",
    "    #greater_is_better=False,\n",
    "    #dataloader_pin_memory=False,\n",
    "    # remove_unused_columns=False, # Removed this line\n",
    "    # Add evaluation metrics\n",
    "    #eval_steps=None,  # Remove this when using epoch strategy,\n",
    "    # Add these for speed\n",
    "    #dataloader_num_workers=0,       # Disable multiprocessing\n",
    "    report_to=None,\n",
    "    #gradient_checkpointing=True # Enable gradient checkpointing\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fine-tuning\n",
    "#print(\"Starting LoRA fine-tuning...\")\n",
    "#trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "#trainer.save_model(\"./flan-t5-large-sql-qlora-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmarking finetuned model on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the saved model under project folder and evaluate the performance on the test set\n",
    "\n",
    "# Load the base model and tokenizer\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", quantization_config=bnb_config)\n",
    "\n",
    "# Load the LoRA weights from checkpoint\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, \"./flan-t5-large-sql-qlora-final\")\n",
    "finetuned_model.to(device)\n",
    "\n",
    "print(\"Model loaded successfully from checkpoint!\")\n",
    "\n",
    "# Run evaluation\n",
    "results, generated_sqls, reference_sqls, test_dataset = evaluate_sql_generation(finetuned_model, tokenizer, test_samples=50)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION RESULTS FOR T5 LARGE FINETUNED WITH LORA MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "metrics_summary = {}\n",
    "for metric_name, scores in results.items():\n",
    "    if scores:\n",
    "        mean_score = np.mean(scores)\n",
    "        std_score = np.std(scores)\n",
    "        metrics_summary[metric_name] = {\n",
    "            'mean': mean_score,\n",
    "            'std': std_score\n",
    "        }\n",
    "        print(f\"{metric_name.replace('_', ' ').title()}: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "\n",
    "# Print some examples\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE GENERATIONS\")\n",
    "print(\"=\"*50)\n",
    "for i in range(min(10, len(generated_sqls))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Actual: {test_dataset[i]['sql']}\")\n",
    "    print(f\"SQL Prompt: {test_dataset[i]['sql_prompt']}\")\n",
    "    print(f\"SQL Context: {test_dataset[i]['sql_context']}\")\n",
    "    print(f\"Generated: {generated_sqls[i]}\")\n",
    "    print(f\"BLEU: {results['bleu_scores'][i]:.4f}\")\n",
    "    print(f\"ROUGE-L: {results['rouge_l_scores'][i]:.4f}\")\n",
    "    print(f\"METEOR: {results['meteor_scores'][i]:.4f}\")\n",
    "    print(f\"GLEU: {results['gleu_scores'][i]:.4f}\")\n",
    "    print(f\"Flesch Reading Ease: {results['flesch_reading_ease_scores'][i]:.4f}\")\n",
    "    print(f\"BERT: {results['bert_scores'][i]:.4f}\")\n",
    "    print(f\"CoSIM: {results['cosim_scores'][i]:.4f}\")\n",
    "    print(f\"Repetition Rate: {results['repetition_rates'][i]:.4f}\")\n",
    "    print(f\"Novelty: {results['novelty_scores'][i]:.4f}\")\n",
    "    print(f\"Diversity: {results['diversity_scores'][i]:.4f}\")   \n",
    "    print(f\"Toxicity: {results['toxicity_scores'][i]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Debugging code\n",
    "\n",
    "# Load your model with proper settings\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, \"./flan-t5-large-sql-qlora-final\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "finetuned_model.eval()\n",
    "finetuned_model.to(device)\n",
    "\n",
    "# Test with different generation parameters\n",
    "def test_generation(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=False,  # Use deterministic generation\n",
    "            temperature=1.0,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test\n",
    "test_prompt = \"Question: What is the average explainability score of creative AI applications in 'Europe' and 'North America' in the 'creative_ai' table? Context: CREATE TABLE creative_ai (application_id INT, name TEXT, region TEXT, explainability_score FLOAT);\"\n",
    "\n",
    "result = test_generation(finetuned_model, tokenizer, test_prompt)\n",
    "print(f\"Generated: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
